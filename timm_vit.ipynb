{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bf199-dd4a-4574-ad6e-07cb396d246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Literal, Optional, Tuple, Union\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plenoptic as po\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from plenoptic.tools.display import clean_up_axes\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "from timm.utils import AttentionExtract\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "timm.layers.set_fused_attn(False)  # Expose all attention internals\n",
    "\n",
    "# needed for the plotting/animating:\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"animation.html\"] = \"html5\"\n",
    "# use single-threaded ffmpeg for animation writer\n",
    "plt.rcParams[\"animation.writer\"] = \"ffmpeg\"\n",
    "plt.rcParams[\"animation.ffmpeg_args\"] = [\"-threads\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebfe22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DINO, with classification head (untrained):\n",
    "model = timm.create_model(\"timm/vit_small_patch16_224.dino\", pretrained=True)\n",
    "\n",
    "# CLIP model, fine-tuned on ImageNet:\n",
    "# model = timm.create_model(\"timm/vit_base_patch32_clip_224.laion2b_ft_in12k_in1k\", pretrained=True)\n",
    "\n",
    "# Set model to eval mode for inference\n",
    "model.eval()\n",
    "\n",
    "# Create Transform\n",
    "transform = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e082f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pretrained_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54600ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    labels = model.pretrained_cfg[\"label_names\"]\n",
    "except KeyError:\n",
    "    # Default to ImageNet if no labels are provided\n",
    "    url = \"https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\"\n",
    "    labels = urllib.request.urlopen(url).read().decode(\"utf-8\").splitlines()\n",
    "resolve_data_config(model.pretrained_cfg, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e37af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and open the image\n",
    "url = \"https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n02110958_pug.JPEG?raw=true\"\n",
    "\n",
    "# url = \"https://github.com/EliSchwartz/imagenet-sample-images/blob/master/n01491361_tiger_shark.JPEG?raw=true\"\n",
    "\n",
    "original_img = Image.open(urlopen(url))\n",
    "\n",
    "img = transforms.PILToTensor()(original_img)\n",
    "\n",
    "if img.shape[0] == 1:\n",
    "    img = img.repeat(3, 1, 1)\n",
    "\n",
    "img = img.unsqueeze(0).float().to(0)\n",
    "\n",
    "if img.max() > 1:\n",
    "    img = img / 255.0\n",
    "\n",
    "print(img.shape)\n",
    "po.imshow(img, as_rgb=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_resnet_ready = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b94bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.imshow(img_resnet_ready, as_rgb=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46c2c11-ab68-44f8-84df-028654b0f28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes, eval_nodes = get_graph_node_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773560d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_nodes[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b8c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntermediateOutputViT(nn.Module):\n",
    "    def __init__(self, model: nn.Module, block_index: int, transform: Optional[Callable] = None):\n",
    "        super().__init__()\n",
    "        self.block_index = block_index\n",
    "        self.attention_output = f\"blocks.{block_index}.attn.attn_drop\"\n",
    "        self.feature_representation = f\"blocks.{block_index}\"\n",
    "        self.extractor = create_feature_extractor(\n",
    "            model, return_nodes=[self.attention_output, self.feature_representation]\n",
    "        )\n",
    "        self.model = model\n",
    "        self.transform = transform\n",
    "\n",
    "    def _extractor(self, x):\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return self.extractor(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._extractor(x)[self.feature_representation]\n",
    "\n",
    "    def plot_representation(\n",
    "        self,\n",
    "        data: Tensor,\n",
    "        ax: Optional[plt.Axes] = None,\n",
    "        figsize: Tuple[float, float] = (15, 15),\n",
    "        ylim: Optional[Union[Tuple[float, float], Literal[False]]] = None,\n",
    "        batch_idx: int = 0,\n",
    "        title: Optional[str] = None,\n",
    "    ) -> Tuple[plt.Figure, List[plt.Axes]]:\n",
    "        feature_representation = data[batch_idx]\n",
    "\n",
    "        class_token_representation = feature_representation[0].squeeze().detach().cpu().numpy()\n",
    "        spatial_representation = feature_representation[1:]\n",
    "\n",
    "        dim_average_representation = spatial_representation.mean(1)\n",
    "        num_patches = int(dim_average_representation.shape[0] ** 0.5)\n",
    "        patch_representation_grid = dim_average_representation.reshape(num_patches, num_patches).detach().cpu().numpy()\n",
    "\n",
    "        # Determine figure layout\n",
    "        if ax is None:\n",
    "            fig, axes = plt.subplots(2, 1, figsize=figsize, gridspec_kw={\"height_ratios\": [1, 1]})\n",
    "        else:\n",
    "            ax = clean_up_axes(ax, False, [\"top\", \"right\", \"bottom\", \"left\"], [\"x\", \"y\"])\n",
    "            gs = ax.get_subplotspec().subgridspec(2, 1, height_ratios=[3, 1])\n",
    "            fig = ax.figure\n",
    "            axes = [fig.add_subplot(gs[0]), fig.add_subplot(gs[1])]\n",
    "\n",
    "        # Plot average error across channels\n",
    "        po.imshow(\n",
    "            ax=axes[0],\n",
    "            image=patch_representation_grid[None, None, ...],\n",
    "            title=f\"{title} - patch tokens\" if title is not None else \"Average Representation across patch tokens\",\n",
    "            vrange=\"auto0\",\n",
    "        )\n",
    "\n",
    "        # Plot the class token representation\n",
    "        axes[1].plot(class_token_representation)\n",
    "        axes[1].set_xlabel(\"Dimension\")\n",
    "        axes[1].set_ylabel(\"Value\")\n",
    "        axes[1].set_title(\"Class Token Representation\") if title is None else axes[1].set_title(\n",
    "            f\"{title} - Class Token Representation\"\n",
    "        )\n",
    "\n",
    "        return fig, axes\n",
    "\n",
    "    def plot_attention(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        ax: Optional[plt.Axes] = None,\n",
    "        figsize: Tuple[float, float] = (15, 15),\n",
    "        ylim: Optional[Union[Tuple[float, float], Literal[False]]] = None,\n",
    "        batch_idx: int = 0,\n",
    "        title: Optional[str] = None,\n",
    "        head_fusion: str = \"mean\",\n",
    "    ) -> Tuple[plt.Figure, List[plt.Axes]]:\n",
    "        attn_map = self._extractor(x)[self.attention_output]\n",
    "\n",
    "        attn_map = attn_map[batch_idx]  # Remove batch dimension\n",
    "\n",
    "        if head_fusion == \"mean_std\":\n",
    "            attn_map = attn_map.mean(0) / attn_map.std(0)\n",
    "        elif head_fusion == \"mean\":\n",
    "            attn_map = attn_map.mean(0)\n",
    "        elif head_fusion == \"max\":\n",
    "            attn_map = attn_map.amax(0)\n",
    "        elif head_fusion == \"min\":\n",
    "            attn_map = attn_map.amin(0)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid head fusion method: {head_fusion}\")\n",
    "\n",
    "        # Use the first token's attention (in most ViTs the class token)\n",
    "        attn_map = attn_map[0]\n",
    "\n",
    "        # Reshape the attention map to 2D\n",
    "        num_patches = int(attn_map.shape[0] ** 0.5)\n",
    "        attn_map = attn_map[1:].reshape(num_patches, num_patches)\n",
    "\n",
    "        # Interpolate to match image size\n",
    "        attn_map = attn_map.clone().detach().unsqueeze(0).unsqueeze(0)\n",
    "        attn_map = F.interpolate(attn_map, size=(64, 64), mode=\"bilinear\", align_corners=False)\n",
    "        attn_map = attn_map.squeeze().cpu().numpy()\n",
    "\n",
    "        # Normalize attention map\n",
    "        attn_map = (attn_map - attn_map.min()) / (attn_map.max() - attn_map.min())\n",
    "\n",
    "        # Determine figure layout\n",
    "        if ax is None:\n",
    "            fig, axes = plt.subplots(1, 1, figsize=figsize)\n",
    "        else:\n",
    "            ax = clean_up_axes(ax, False, [\"top\", \"right\", \"bottom\", \"left\"], [\"x\", \"y\"])\n",
    "            gs = ax.get_subplotspec().subgridspec(1, 1)\n",
    "            fig = ax.figure\n",
    "            axes = fig.add_subplot(gs[0])\n",
    "\n",
    "        # Plot the attention map\n",
    "\n",
    "        po.imshow(\n",
    "            attn_map[None, None, ...],\n",
    "            ax=axes,\n",
    "            title=title if title is not None else f\"Self-attention map for CLS token @ block {self.block_index}\",\n",
    "        )\n",
    "\n",
    "        return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f434ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = IntermediateOutputViT(model, 11, transform)\n",
    "test_model.to(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3664969",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.tools.remove_grad(test_model)\n",
    "\n",
    "test_model.eval()\n",
    "\n",
    "po.tools.validate.validate_model(test_model, device=0, image_shape=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed01166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(img.to(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f190521",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.plot_representation(test_model(img.to(0)), title=\"Representation at block 11\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6252ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.plot_attention(img.to(0), figsize=(5, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04357af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass_gaussian(img, kernel_size=11, sigma=5):\n",
    "    \"\"\"Applies a Gaussian blur to low-pass filter the image.\"\"\"\n",
    "    blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)\n",
    "    return blur(img)\n",
    "\n",
    "\n",
    "met = po.synth.Metamer(\n",
    "    img,\n",
    "    test_model,\n",
    "    # initial_image=low_pass_gaussian(img)\n",
    ")\n",
    "optim = torch.optim.AdamW([met.metamer], lr=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, \"min\", patience=50, factor=0.5, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e01f49",
   "metadata": {},
   "source": [
    "To synthesize the model metamer, we use the synthesize method. Setting the `store_progress` arg stores copies of the model metamer over time, which will allow us to visualize synthesis progress after the fact:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2415dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "met.synthesize(\n",
    "    5000, store_progress=10, optimizer=optim, scheduler=scheduler, stop_criterion=1e-6, stop_iters_to_check=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a83f13",
   "metadata": {},
   "source": [
    "The plot on the left shows the model metamer, the middle plot shows the synthesis loss, and the plot on the left shows the model representation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "po.synth.metamer.plot_synthesis_status(met, ylim=False, iteration=-1, zoom=1, figsize=(25, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e234d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "po.imshow(transform(img), ax=axes[0], title=\"Input image\")\n",
    "\n",
    "test_model.plot_attention(img.to(0), ax=axes[1], title=\"Attention map for original image\")\n",
    "\n",
    "test_model.plot_attention(met.metamer, ax=axes[2], title=\"Attention map for metamer\")\n",
    "\n",
    "attn_difference = (\n",
    "    test_model._extractor(met.metamer)[test_model.attention_output]\n",
    "    - test_model._extractor(img.to(0))[test_model.attention_output]\n",
    ")\n",
    "\n",
    "# Squeeze batch out and average over heads\n",
    "attn_difference = attn_difference.squeeze().mean(0)\n",
    "\n",
    "# Use the first token's attention (in most ViTs the class token)\n",
    "attn_map_diff = attn_difference[0]\n",
    "\n",
    "print(attn_map_diff.shape)\n",
    "\n",
    "# Reshape the attention map to 2D\n",
    "num_patches = int(attn_map_diff.shape[0] ** 0.5)\n",
    "attn_map_diff = attn_map_diff[1:].reshape(num_patches, num_patches)\n",
    "\n",
    "# Interpolate to match image size\n",
    "attn_map_diff = attn_map_diff.clone().detach().unsqueeze(0).unsqueeze(0)\n",
    "attn_map_diff = F.interpolate(attn_map_diff, size=(64, 64), mode=\"bilinear\", align_corners=False)\n",
    "attn_map_diff = attn_map_diff.squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "po.imshow(attn_map_diff[None, None, ...], ax=axes[3], title=\"Difference in attention maps\")\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Attention map comparison between original image and metamer for representations @ block {test_model.block_index}\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class label for the original model:\")\n",
    "print(labels[torch.argmax(model(transform(img).to(0)).squeeze()).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa46a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class label for the metamer:\")\n",
    "print(labels[torch.argmax(model(transform(met.metamer).to(0)).squeeze()).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for both images\n",
    "with torch.no_grad():\n",
    "    original_logits = model(transform(img).to(0)).squeeze()\n",
    "    metamer_logits = model(transform(met.metamer).to(0)).squeeze()\n",
    "\n",
    "# Convert to probabilities\n",
    "original_probs = F.softmax(original_logits, dim=0)\n",
    "metamer_probs = F.softmax(metamer_logits, dim=0)\n",
    "\n",
    "# Get top predictions for both images (for labeling key points)\n",
    "k = 10\n",
    "top_original = torch.topk(original_probs, k)\n",
    "top_metamer = torch.topk(metamer_probs, k)\n",
    "combined_top_indices = torch.unique(torch.cat([top_original.indices, top_metamer.indices]))\n",
    "\n",
    "# Create a figure with two plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot 1: Scatter plot of probabilities\n",
    "axs[0].scatter(\n",
    "    original_probs[combined_top_indices].cpu().numpy(),\n",
    "    metamer_probs[combined_top_indices].cpu().numpy(),\n",
    "    alpha=0.7,\n",
    "    s=100,\n",
    ")\n",
    "\n",
    "# Add diagonal line\n",
    "max_prob = max(original_probs.max().item(), metamer_probs.max().item())\n",
    "axs[0].plot([0, max_prob], [0, max_prob], \"k--\", alpha=0.5)\n",
    "\n",
    "# Label key points\n",
    "for idx in combined_top_indices:\n",
    "    axs[0].annotate(labels[idx], (original_probs[idx].item(), metamer_probs[idx].item()), fontsize=9)\n",
    "\n",
    "axs[0].set_xlabel(\"Original Image Probability\")\n",
    "axs[0].set_ylabel(\"Metamer Probability\")\n",
    "axs[0].set_title(\"Class Probabilities Comparison\")\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bar chart of top predictions\n",
    "x = np.arange(len(combined_top_indices))\n",
    "width = 0.35\n",
    "\n",
    "axs[1].bar(x - width / 2, original_probs[combined_top_indices].cpu().numpy(), width, label=\"Original\")\n",
    "axs[1].bar(x + width / 2, metamer_probs[combined_top_indices].cpu().numpy(), width, label=\"Metamer\")\n",
    "\n",
    "axs[1].set_xticks(x)\n",
    "axs[1].set_xticklabels([labels[i] for i in combined_top_indices], rotation=45, ha=\"right\")\n",
    "axs[1].set_ylabel(\"Probability\")\n",
    "axs[1].set_title(\"Top Class Probabilities\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
